{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65165768",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavideScassola/PML2024/blob/main/./Notebooks/06_bayesian_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6: Bayesian Linear Regression & Classification\n",
    "\n",
    "Probabilistic Machine Learning -- Spring 2024, UniTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore linear regression and classification from a Bayesian point of view. In both cases, we start implementing the classical formulation and then we switch to the Bayesian version. We can then compare the methods on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "The first model that we implement is the well known linear regression model and we use the \"linear algebra view\" that we studied during the lessons.\n",
    "\n",
    "As a case study, we use the [Kaggle dataset](https://www.kaggle.com/datasets/budincsevity/szeged-weather/data) about Weather in Szeged (Hungary) between 2006 and 2016 and we try to predict the apparent temperature given some predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherHistory = pd.read_csv('https://raw.githubusercontent.com/DavideScassola/PML2024/main/Notebooks/data/weatherHistory.csv')\n",
    "weatherHistory.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherHistory.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's reduce the size of the dataset by considering one row for each day (corresponding to 16:00), only in 2015\n",
    "#Training: April, Test: May\n",
    "weatherHistory['Formatted Date'] = pd.to_datetime(weatherHistory['Formatted Date'], utc=True)\n",
    "data = weatherHistory[((weatherHistory['Formatted Date'].dt.hour == 16) & (weatherHistory['Formatted Date'].dt.year == 2014) & (weatherHistory['Formatted Date'].dt.month == 4))].reset_index(drop=True)\n",
    "data.info()\n",
    "data_test = weatherHistory[((weatherHistory['Formatted Date'].dt.hour == 16) & (weatherHistory['Formatted Date'].dt.year == 2014) & (weatherHistory['Formatted Date'].dt.month == 5))].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[\"Apparent Temperature (C)\"], data[\"Humidity\"], label = \"train\")\n",
    "plt.scatter(data_test[\"Apparent Temperature (C)\"], data_test[\"Humidity\"], label = \"test\")\n",
    "\n",
    "plt.xlabel(\"Humidity\")\n",
    "plt.ylabel(\"Apparent Temperature (C)\")\n",
    "plt.title(\"Apparent Temperature vs Humidity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform the linear regression, we follow the maximum likelihood approach:\n",
    "$$\\theta_{ML} = \\text{argmax } p(\\underline{y} | \\underline{x}, \\theta)$$\n",
    "\n",
    "and we define our parametric model as:\n",
    "$$p(y|x, \\theta) = \\mathcal{N}(y|f(x,w), \\beta^{-1})$$\n",
    "\n",
    "where \n",
    "\n",
    "$$f(x,w) = w_0 \\phi_0(x) + \\ldots + w_{M-1}\\phi_{M-1}(x)$$\n",
    "\n",
    "with $\\phi$ basis functions.\n",
    "\n",
    "The weighs can be computed by:\n",
    "\n",
    "$$w_M = \\left( \\Phi^T \\Phi \\right)^{-1} \\Phi^T \\underline{y}$$\n",
    "\n",
    "where $\\Phi$ is the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_basis(x):\n",
    "    return x\n",
    "\n",
    "def linear_regression(x,y,basis_function):\n",
    "    \n",
    "    N = len(y)\n",
    "    train_features = basis_function(x)\n",
    "   \n",
    "    X = np.hstack((np.ones((N,1)), train_features))\n",
    "    print(f'matrice delle features: {X}')\n",
    "   \n",
    "    A = np.linalg.inv(X.T @ X)\n",
    "    D = A @ X.T\n",
    "    w = D @ y\n",
    "   \n",
    "    return np.dot(X, w), w\n",
    "\n",
    "def predict_linear_regression(x,N,w,basis_function):\n",
    "    \n",
    "    train_features = basis_function(x)\n",
    "    X = np.hstack((np.ones((N,1)), train_features))\n",
    "    return np.dot(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the linear regression on the dataset and compute:\n",
    "\n",
    "- Mean Squared Error (MSE): $$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$\n",
    "- Log Likelihood: $$\\sum_i -\\frac{1}{2} \\log(2\\pi\\sigma_i^2) - \\frac{(y_i - \\mu_i)^2}{2\\sigma_i^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this case x and y are row vectors, so we need to convert them to column vectors\n",
    "x = data[\"Humidity\"].to_numpy().reshape(-1,1)\n",
    "y = data[\"Apparent Temperature (C)\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "print(\"X (Humidity) vs Y (Apparent Temperature):\")\n",
    "\n",
    "beta = 0.01\n",
    "print(np.sqrt(1/beta))\n",
    "\n",
    "y_pred_linear, w = linear_regression(x,y,linear_basis)\n",
    "print(\"X (Humidity) vs Y (Apparent Temperature) vs Y (prediction):\")\n",
    "print(np.hstack([x, y, y_pred_linear]))\n",
    "print(\"w:\")\n",
    "print(w)\n",
    "\n",
    "plt.scatter(data[\"Humidity\"], data[\"Apparent Temperature (C)\"], label = 'Data')\n",
    "plt.scatter(data[\"Humidity\"].to_numpy(), y_pred_linear, label = 'Predictions')\n",
    "plt.xlabel(\"Humidity\")\n",
    "plt.ylabel(\"Apparent Temperature (C)\")\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\")\n",
    "print(np.mean((y_pred_linear.T-y)**2))\n",
    "print(\"Log Likelihood:\")\n",
    "print(-len(y)/2*np.log(2*np.pi)-len(y)/2*np.log(1/beta) - 1/2*beta*np.sum((y_pred_linear.T-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see now on the test set\n",
    "\n",
    "x_test = data_test[\"Humidity\"].to_numpy().reshape(-1,1)\n",
    "y_test = data_test[\"Apparent Temperature (C)\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "y_test_pred= predict_linear_regression(x_test,len(x_test),w,linear_basis)\n",
    "\n",
    "plt.scatter(data_test[\"Humidity\"], data_test[\"Apparent Temperature (C)\"], label = 'Data')\n",
    "plt.scatter(data_test[\"Humidity\"].to_numpy(), y_test_pred, label = 'Predictions')\n",
    "plt.xlabel(\"Humidity\")\n",
    "plt.ylabel(\"Apparent Temperature (C)\")\n",
    "plt.title(\"Linear Regression (test data)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\")\n",
    "print(np.mean((y_test_pred.T-y_test)**2))\n",
    "print(\"Log Likelihood:\")\n",
    "print(-len(y_test)/2*np.log(2*np.pi*1/beta) - 1/2*np.sum((y_test_pred.T-y_test)**2*beta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to change the basis function\n",
    "\n",
    "def square_basis(x):\n",
    "    return  x**2   \n",
    "def square_basis_5(x):\n",
    "    return  x + x**2   \n",
    "def square_basis_2(x):\n",
    "    return np.hstack((x,x**2,x**3))\n",
    "def square_basis_3(x):\n",
    "    return np.hstack((x,x**2,x**3,x**4,x**5,x**6,x**7,x**8,x**9,x**10,x**11,x**12,x**13,x**14,x**15,x**16,x**17,x**18,x**19))\n",
    "\n",
    "print(square_basis_2(2))\n",
    "\n",
    "y_pred_1, w = linear_regression(x,y,linear_basis)\n",
    "y_pred_2, w = linear_regression(x,y,square_basis)\n",
    "y_pred_3, w = linear_regression(x,y,square_basis_5)\n",
    "y_pred_4, w = linear_regression(x,y,square_basis_3)\n",
    "# print('w:')\n",
    "# print(w)\n",
    "\n",
    "plt.scatter(data[\"Humidity\"], data[\"Apparent Temperature (C)\"], label = 'Data')\n",
    "plt.scatter(data[\"Humidity\"].to_numpy(), y_pred_1, label = 'Predictions linear')\n",
    "plt.scatter(data[\"Humidity\"].to_numpy(), y_pred_2, label = 'Predictions square')\n",
    "plt.scatter(data[\"Humidity\"].to_numpy(), y_pred_3, label = 'Predictions square_2')\n",
    "# plt.scatter(data[\"Humidity\"].to_numpy(), y_pred_4, label = 'Predictions square_3')\n",
    "plt.xlabel(\"Humidity\")\n",
    "plt.ylabel(\"Apparent Temperature (C)\")\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print(\"MSE:\")\n",
    "# print(np.mean((y_pred.T-y)**2))\n",
    "# print(\"Log Likelihood:\")\n",
    "# print(-len(y)/2*np.log(2*np.pi)-len(y)/2*np.log(1/beta) - 1/2*beta*np.sum((y_pred.T-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also consider more predictors\n",
    "\n",
    "x = data[[\"Temperature (C)\", \"Humidity\", \"Wind Speed (km/h)\"]].to_numpy()\n",
    "y = data[\"Apparent Temperature (C)\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "y_pred, w = linear_regression(x,y,linear_basis)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(data[\"Temperature (C)\"], data[\"Humidity\"], s = data[\"Wind Speed (km/h)\"]*2, c = y_pred)\n",
    "legend1 = ax.legend(*scatter.legend_elements(), loc = 'upper right', title=\"Predicted App. Temp.\")\n",
    "ax.add_artist(legend1)\n",
    "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.6)\n",
    "legend2 = ax.legend(handles, labels, loc = 'lower left', title=\"Wind Speed\")\n",
    "\n",
    "plt.title(\"Linear Regression with more predictors\")\n",
    "plt.ylabel(\"Humidity\")\n",
    "plt.xlabel(\"Temperature (C)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\")\n",
    "print(np.mean((y_pred.T-y)**2))\n",
    "print(\"Log Likelihood:\")\n",
    "print(-len(y)/2*np.log(2*np.pi)-len(y)/2*np.log(1/beta) - 1/2*beta*np.sum((y_pred.T-y)**2))\n",
    "print(\"Weights:\")\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression\n",
    "\n",
    "We now consider a prior distribution on the weights, which in this case is:\n",
    "\n",
    "$$p(w|\\alpha) = \\mathcal{N}(w| 0, \\alpha^{-1} I)$$\n",
    "\n",
    "So, we can use Bayes theorem to compute the posterior:\n",
    "\n",
    "$$    p(w| \\underline{x}, \\underline{y}, \\alpha, \\beta) = \\frac{p(\\underline{y}| \\underline{x}, w, \\beta) p(w|\\alpha)}{p(\\underline{y}| \\underline{x}, \\alpha, \\beta)}$$\n",
    "\n",
    "and notice that it is a Gaussian distribution:\n",
    "\n",
    "$$p(w|\\underline{x}, \\underline{y}, \\alpha, \\beta) = \\mathcal{N} (w | m_N, S_N)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$m_N = \\beta S_N \\Phi^T \\underline{y}$$\n",
    "$$S_N^{-1} = \\alpha I + \\beta \\Phi^T \\Phi$$\n",
    "\n",
    "Once we have the posterior distribution, we can calculate the predictive distribution as:\n",
    "\n",
    "$$p(y|x, \\underline{x} , \\underline{y}, \\alpha, \\beta)  = \\mathcal{N} \\left(y| m_N^T \\phi(x), \\sigma_N^2(x) \\right)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\sigma_N^2(x) = \\frac{1}{\\beta} + \\phi^T(x) S_N \\phi(x)$$\n",
    "\n",
    "For simplicity and for visualizing the result, we test on a single predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Regression with alpha and beta fixed and a gaussian prior\n",
    "\n",
    "def bayesian_linear_regression(x,y,basis_function, alpha, beta):\n",
    "    N = len(y)\n",
    "    train_features = basis_function(x)\n",
    "    \n",
    "    X = np.hstack((np.ones((N,1)), train_features))\n",
    "    # print(f'matrice delle features: {X.T}')\n",
    "    \n",
    "    SN_inv = alpha*np.eye(X.shape[1]) + beta*X.T @ X\n",
    "    # print(f' alpha * I : {alpha*np.eye(X.shape[1])}')\n",
    "    SN = np.linalg.inv(SN_inv)  # covariance posterior distribution\n",
    "    mN = beta * SN @ X.T @ y    # mean posterior distribution\n",
    "    \n",
    "    return np.dot(mN.T, X.T), np.sqrt(1/beta + np.diag(X @ SN @ X.T)), mN, SN\n",
    "\n",
    "def predict_bayesian_linear_regression(mN, SN, N, x, basis_function, beta):\n",
    "    train_features = basis_function(x)\n",
    "    X = np.hstack((np.ones((N,1)), train_features))\n",
    "    mean = np.dot(mN.T, X.T)[0]                       # mean predictive distribution\n",
    "    sigma = np.sqrt(1/beta + np.diag(X @ SN @ X.T))   # variance predictive distribution\n",
    "    return mean, sigma\n",
    "\n",
    "def plot_bayesian_LR(mN, SN, N, x, basis_function, beta):\n",
    "    mean, sigma = predict_bayesian_linear_regression(mN, SN, N, x, basis_function, beta)\n",
    "    plt.plot(x.T[0],mean)\n",
    "    plt.fill_between(x.T[0], mean-sigma, mean+sigma, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"Humidity\"].to_numpy().reshape(-1,1)\n",
    "y = data[\"Apparent Temperature (C)\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "x_test = data_test[\"Humidity\"].to_numpy().reshape(-1,1)\n",
    "y_test = data_test[\"Apparent Temperature (C)\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "alpha = 0.002\n",
    "\n",
    "\n",
    "y_pred_bayes, sigma_pred, mN, SN = bayesian_linear_regression(x,y,linear_basis, alpha, beta)\n",
    "print(\"MSE (training set):\")\n",
    "print(np.mean((y_pred_bayes-y)**2))\n",
    "print(\"Log Likelihood (training set):\")\n",
    "print(-1/2*np.sum(np.log(2*np.pi*sigma_pred**2)) - 1/2*np.sum(((y_pred_bayes-y)**2)/(sigma_pred**2)))\n",
    "\n",
    "mean, sigma = predict_bayesian_linear_regression(mN, SN, len(x_test), x_test, linear_basis, beta)\n",
    "# print(f' mN: {mN}')\n",
    "# print(f' SN: {SN}')\n",
    "# print(f' X_test.T: {x_test.T}')\n",
    "# print(np.hstack([x_test, mean.reshape(-1,1), sigma.reshape(-1,1)]))\n",
    "print(\"MSE (test set):\")\n",
    "print(np.mean((mean-y_test)**2))\n",
    "print(\"Log Likelihood (test set):\")\n",
    "print(-1/2*np.sum(np.log(2*np.pi*sigma**2)) - 1/2*np.sum(((mean-y_test)**2)/(sigma**2)))\n",
    "\n",
    "\n",
    "\n",
    "#FOR 1D features, we can plot the regression\n",
    "xrange=np.linspace(0, 1, 100).reshape(-1,1)\n",
    "plot_bayesian_LR(mN, SN, len(xrange), xrange, linear_basis, beta)\n",
    "\n",
    "plt.scatter(data[\"Humidity\"], data[\"Apparent Temperature (C)\"], label='Data training')\n",
    "plt.scatter(data_test[\"Humidity\"], data_test[\"Apparent Temperature (C)\"], label='Data test')\n",
    "\n",
    "plt.xlabel(\"Humidity\")\n",
    "plt.ylabel(\"Apparent Temperature (C)\")\n",
    "plt.title(\"Bayesian Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given observations $(\\textbf{x}_i, y_i)$ with $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{0,1\\}$ we want to model $p(y | \\textbf{x})$.\n",
    "\n",
    "**Notation**\n",
    "\n",
    "$\\textbf{X}$: the dataset of input features $\\textbf{x}_i$\n",
    "\n",
    "$\\textbf{Y}$: the dataset of targets $y_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " total data\n",
      "     feature_0  feature_1  target\n",
      "0    0.441227  -0.330870       0\n",
      "1    2.430771  -0.252092       0\n",
      "2    0.109610   1.582481       0\n",
      "3   -0.909232  -0.591637       1\n",
      "4    0.187603  -0.329870       0\n",
      "..        ...        ...     ...\n",
      "95   0.249817   1.147202       0\n",
      "96   0.835944   0.287404       0\n",
      "97  -0.995596   0.906889       1\n",
      "98   0.024211  -0.239982       1\n",
      "99   0.910111   0.617845       0\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "train data\n",
      "    feature_0  feature_1  target\n",
      "0  -0.063461  -0.500996       0\n",
      "1   1.579113  -0.409813       0\n",
      "2  -0.337246   1.713631       0\n",
      "3  -1.178406  -0.802822       1\n",
      "test data\n",
      "     feature_0  feature_1  target\n",
      "0   -0.272854  -0.499838       0\n",
      "1   -1.412491  -0.355163       1\n",
      "2   -0.723991   0.580467       1\n",
      "3   -1.802196  -0.928456       0\n",
      "4    0.522852   2.031759       0\n",
      "..        ...        ...     ...\n",
      "91  -0.221490   1.209813       0\n",
      "92   0.262418   0.214631       0\n",
      "93  -1.249708   0.931661       1\n",
      "94  -0.407752  -0.395796       1\n",
      "95   0.323650   0.597104       0\n",
      "\n",
      "[96 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "# URL of the file to download\n",
    "url = 'https://raw.githubusercontent.com/DavideScassola/PML2024/main/Notebooks/data/logistic_regression_data.npz'\n",
    "\n",
    "# Send a HTTP request to the URL of the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the 'data/' directory exists\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Write the file\n",
    "with open('data/logistic_regression_data.npz', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# loading the dataset\n",
    "X = np.load('data/logistic_regression_data.npz')['X']\n",
    "y = np.load('data/logistic_regression_data.npz')['y']\n",
    "\n",
    "# Crea un DataFrame\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "df['target'] = y\n",
    "print(f' total data\\n {df}')\n",
    "\n",
    "\n",
    "# let's split it into train and test set and normalize it\n",
    "def split_and_standardize(*, X, y, ratio=0.8):\n",
    "    \n",
    "    # Splitting train and test sets\n",
    "    cut = int(ratio * len(y))\n",
    "    X_train = X[:cut]\n",
    "    y_train = y[:cut]\n",
    "    \n",
    "    X_test = X[cut:]\n",
    "    y_test = y[cut:]\n",
    "    \n",
    "    # Standardizing data\n",
    "    x_mean = X_train.mean(axis=0)\n",
    "    x_std = X_train.std(axis=0)\n",
    "    \n",
    "    X_train = (X_train - x_mean) / x_std\n",
    "    X_test = (X_test - x_mean) / x_std\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_and_standardize(X=X, y=y, ratio=0.04)\n",
    "\n",
    "df = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(X_train.shape[1])])\n",
    "df['target'] = y_train\n",
    "print(f'train data\\n {df}')\n",
    "\n",
    "df = pd.DataFrame(X_test, columns=[f'feature_{i}' for i in range(X_test.shape[1])])\n",
    "df['target'] = y_test\n",
    "print(f'test data\\n {df}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "_ = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression $p(y | \\textbf{x})$ is modelled as:\n",
    "\n",
    "$$p_{\\textbf{w}}(y |  \\textbf{x}) = Bernoulli(y; f(\\textbf{w}^{T} \\textbf{x})) $$\n",
    "\n",
    "where $f : \\mathbb{R} \\rightarrow [0,1]$ is usually chosen to be the Logit $ \\left( ({1 + e^{-k}})^{-1} \\right)$ or the Probit.\n",
    "\n",
    "Why $\\textbf{w}^{T} \\textbf{x}$ and not $\\textbf{w}^{T} \\textbf{x} + h$? For simplicity we prefer to avoid explicitly modelling the bias and we equivalently add a \"1\" as a feature to each data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: [[ 1.         -0.06346129 -0.50099568]\n",
      " [ 1.          1.57911298 -0.40981329]\n",
      " [ 1.         -0.33724598  1.71363139]\n",
      " [ 1.         -1.17840571 -0.80282242]]\n",
      "X_test: [[ 1.00000000e+00 -2.72854366e-01 -4.99837999e-01]\n",
      " [ 1.00000000e+00 -1.41249090e+00 -3.55163115e-01]\n",
      " [ 1.00000000e+00 -7.23990686e-01  5.80467288e-01]\n",
      " [ 1.00000000e+00 -1.80219550e+00 -9.28455853e-01]\n",
      " [ 1.00000000e+00  5.22852211e-01  2.03175908e+00]\n",
      " [ 1.00000000e+00 -1.67537539e+00  6.28358237e-01]\n",
      " [ 1.00000000e+00 -1.23733356e+00 -1.10979984e+00]\n",
      " [ 1.00000000e+00 -1.14756676e+00 -6.07062722e-01]\n",
      " [ 1.00000000e+00  3.94923971e-01  7.06572144e-01]\n",
      " [ 1.00000000e+00 -3.78910553e-01 -5.38544518e-01]\n",
      " [ 1.00000000e+00 -4.25024967e-01 -2.40637023e-01]\n",
      " [ 1.00000000e+00  2.27007356e-01 -8.49045540e-01]\n",
      " [ 1.00000000e+00 -4.32854786e-01 -2.35008493e-01]\n",
      " [ 1.00000000e+00 -4.70926041e-01  1.70432608e-01]\n",
      " [ 1.00000000e+00 -2.64551383e-01  1.42700669e+00]\n",
      " [ 1.00000000e+00 -4.99465053e-01  1.68938397e+00]\n",
      " [ 1.00000000e+00 -6.80253576e-01 -6.70981686e-01]\n",
      " [ 1.00000000e+00 -3.44570446e-01  2.93378924e-01]\n",
      " [ 1.00000000e+00 -2.05147299e-01  1.37736881e+00]\n",
      " [ 1.00000000e+00  5.12905315e-01  4.54268058e-01]\n",
      " [ 1.00000000e+00 -7.05420644e-01 -2.34483857e-01]\n",
      " [ 1.00000000e+00  7.39166117e-01  1.38065941e-01]\n",
      " [ 1.00000000e+00 -1.50991915e+00 -9.16170800e-01]\n",
      " [ 1.00000000e+00 -9.04537200e-01  1.21560384e+00]\n",
      " [ 1.00000000e+00 -5.16215210e-01  2.49795582e+00]\n",
      " [ 1.00000000e+00  1.14367092e-01  2.64320760e-02]\n",
      " [ 1.00000000e+00 -7.87458933e-01  1.00723162e+00]\n",
      " [ 1.00000000e+00 -6.26472212e-01 -1.07191653e+00]\n",
      " [ 1.00000000e+00  4.13121182e-02 -1.03259563e-01]\n",
      " [ 1.00000000e+00  5.53952461e-01 -2.03208105e-01]\n",
      " [ 1.00000000e+00 -2.78870867e+00  7.95633039e-01]\n",
      " [ 1.00000000e+00 -1.97800971e+00  1.66302150e+00]\n",
      " [ 1.00000000e+00  1.07598487e+00 -6.12298441e-01]\n",
      " [ 1.00000000e+00 -1.38932223e+00 -1.73508375e+00]\n",
      " [ 1.00000000e+00  2.92726471e-01 -3.51970456e-01]\n",
      " [ 1.00000000e+00 -9.21748309e-01 -3.99775307e-01]\n",
      " [ 1.00000000e+00  1.29654481e+00  2.83518356e-01]\n",
      " [ 1.00000000e+00  1.87934540e-01  7.81212464e-01]\n",
      " [ 1.00000000e+00  4.13071814e-01  1.11044477e+00]\n",
      " [ 1.00000000e+00 -1.01430384e+00 -3.67098020e-01]\n",
      " [ 1.00000000e+00 -1.05582663e+00 -9.41169612e-01]\n",
      " [ 1.00000000e+00  5.14692547e-01 -6.98788650e-01]\n",
      " [ 1.00000000e+00 -4.93087889e-01 -9.19945404e-01]\n",
      " [ 1.00000000e+00 -9.17655658e-01  7.94326891e-01]\n",
      " [ 1.00000000e+00 -7.95488812e-01 -6.76061510e-01]\n",
      " [ 1.00000000e+00 -2.02573105e-02  4.61267135e-01]\n",
      " [ 1.00000000e+00  2.26486647e-01  7.96261795e-02]\n",
      " [ 1.00000000e+00 -1.87563486e+00  6.11516000e-01]\n",
      " [ 1.00000000e+00 -1.63178989e-02  1.06042650e+00]\n",
      " [ 1.00000000e+00 -1.12658771e+00  2.79970657e+00]\n",
      " [ 1.00000000e+00 -1.44520255e+00  7.23392337e-01]\n",
      " [ 1.00000000e+00 -3.89799453e-01 -6.81863230e-01]\n",
      " [ 1.00000000e+00  2.46061494e-01 -4.43770838e-01]\n",
      " [ 1.00000000e+00 -2.35603782e+00 -1.46910462e+00]\n",
      " [ 1.00000000e+00 -4.98961375e-02  2.47232831e+00]\n",
      " [ 1.00000000e+00 -1.65067304e+00 -1.37640324e-01]\n",
      " [ 1.00000000e+00  7.69990458e-01  5.84401166e-01]\n",
      " [ 1.00000000e+00 -7.37854135e-01 -1.76384723e+00]\n",
      " [ 1.00000000e+00 -1.89826501e+00 -9.83697645e-01]\n",
      " [ 1.00000000e+00 -7.31900450e-01 -2.72129050e+00]\n",
      " [ 1.00000000e+00 -1.82604431e+00  2.33434076e-01]\n",
      " [ 1.00000000e+00 -7.65405758e-01 -9.91712826e-01]\n",
      " [ 1.00000000e+00 -7.64189501e-01 -1.05860725e+00]\n",
      " [ 1.00000000e+00  3.38020389e-01  1.78015402e+00]\n",
      " [ 1.00000000e+00  1.28424401e+00  7.03535651e-01]\n",
      " [ 1.00000000e+00  1.89111555e-01  1.57544732e+00]\n",
      " [ 1.00000000e+00  1.00752920e+00  1.57788888e+00]\n",
      " [ 1.00000000e+00  5.73125187e-01 -8.52611523e-01]\n",
      " [ 1.00000000e+00 -1.74845695e+00  8.97242272e-01]\n",
      " [ 1.00000000e+00 -5.05208647e-01 -1.82463782e-01]\n",
      " [ 1.00000000e+00 -1.16204913e+00 -1.62382574e+00]\n",
      " [ 1.00000000e+00  7.29893436e-01  4.20308878e-01]\n",
      " [ 1.00000000e+00 -1.30703492e+00  3.33869338e-01]\n",
      " [ 1.00000000e+00 -1.75435673e-01  4.85992979e-01]\n",
      " [ 1.00000000e+00  1.41605597e+00 -1.58193086e-01]\n",
      " [ 1.00000000e+00 -1.48420164e+00 -7.57301640e-02]\n",
      " [ 1.00000000e+00 -9.61374514e-01  5.16918390e-01]\n",
      " [ 1.00000000e+00 -2.53915657e-01  2.83035028e-01]\n",
      " [ 1.00000000e+00 -8.95899683e-01  3.60789769e-01]\n",
      " [ 1.00000000e+00 -8.49009439e-01  4.80680082e-01]\n",
      " [ 1.00000000e+00 -6.76251138e-01 -1.41253723e+00]\n",
      " [ 1.00000000e+00 -1.61573943e-02 -9.35302033e-01]\n",
      " [ 1.00000000e+00  7.64333059e-01  3.94677241e-01]\n",
      " [ 1.00000000e+00 -4.16011809e-02 -7.24285376e-04]\n",
      " [ 1.00000000e+00 -4.76741452e-01 -2.88053337e+00]\n",
      " [ 1.00000000e+00  5.81288054e-01 -1.06009719e+00]\n",
      " [ 1.00000000e+00  3.61752898e-01 -8.57077067e-01]\n",
      " [ 1.00000000e+00 -5.45905921e-01 -3.77517508e-01]\n",
      " [ 1.00000000e+00 -1.28512556e+00 -3.16773113e-01]\n",
      " [ 1.00000000e+00 -3.44671726e-02 -5.97421472e-01]\n",
      " [ 1.00000000e+00 -1.47137189e+00 -7.81500734e-01]\n",
      " [ 1.00000000e+00 -2.21490195e-01  1.20981343e+00]\n",
      " [ 1.00000000e+00  2.62418017e-01  2.14631350e-01]\n",
      " [ 1.00000000e+00 -1.24970805e+00  9.31661254e-01]\n",
      " [ 1.00000000e+00 -4.07751772e-01 -3.95795999e-01]\n",
      " [ 1.00000000e+00  3.23650228e-01  5.97103629e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add the \"bias feature\"\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "print(f'X_train: {X_train}')\n",
    "print(f'X_test: {X_test}')\n",
    "\n",
    "def logit(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def likelihood(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "    p = logit(X @ w)  # (n, d) @ (d, )\n",
    "    return np.where(y == 1, p, 1 - p).prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood fitting (frequentist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit our model $p_{\\textbf{w}}(y |  \\textbf{x})$ by finding parameters $w$ that maximize the likelihood according to our data. For numerical reasons we equivalently maximize the log-likelihood:\n",
    "\n",
    "$$ \\log{p_{\\textbf{w}}(Y |  \\textbf{X})} =  \\log{ \\prod_i {        p_{\\textbf{w}}(y_i |  \\textbf{x}_i)             }} = \\sum_i \\log{p_{\\textbf{w}}(y_i |  \\textbf{x}_i)} = \\sum_i \\log{Bernoulli(y_i; f(\\textbf{w}^{T} \\textbf{x}_i))}$$\n",
    "\n",
    "We can find $w_{ML} = \\argmax_{\\textbf{w}}  \\log{p_{\\textbf{w}}(Y |  \\textbf{X})}$ using numerical optimization, in particular we can use gradient descent:\n",
    "\n",
    "$$ w_{n+1} = w_{n} + \\eta \\nabla_{\\textbf{w}} \\log{p_{\\textbf{w}}(Y |  \\textbf{X})} $$\n",
    "\n",
    "where $\\eta$ is the learning rate. How do we compute the gradient? We can compute it analyticallly, but we can also use automatic differentiation with `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|| 5000/5000 [00:01<00:00, 4144.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.09124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def log_likelihood(*, X: torch.Tensor, y: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    p = torch.sigmoid(X @ w)\n",
    "    return torch.where(y == 1, torch.log(p + 1e-8), torch.log(1 - p + 1e-8)).sum()\n",
    "\n",
    "\n",
    "def gradient_descent_optimization(*, loss_function, lr: float, n_iter: int, initial_guess: np.ndarray) -> np.ndarray:\n",
    "    w = torch.tensor(initial_guess, dtype=torch.float32, requires_grad=True)\n",
    "    for _ in tqdm(range(n_iter)):\n",
    "        loss = loss_function(w)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w -= lr * w.grad\n",
    "        w.grad = None\n",
    "    print(f\"loss={ loss.item():.4g}\")\n",
    "    return w.cpu().detach().numpy()\n",
    "\n",
    "X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "loss_function = lambda w: -log_likelihood(X=X_tensor,\n",
    "                                          y=y_tensor,\n",
    "                                          w=w)\n",
    "\n",
    "w_mle = gradient_descent_optimization(loss_function=loss_function,\n",
    "                                      initial_guess=np.zeros(X_train.shape[1]),\n",
    "                                      lr=1e-2,\n",
    "                                      n_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x00000217853B6340>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tomma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\tomma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      " [[3 0]\n",
      " [0 1]]\n",
      "mean cross entropy: 0.0228\n"
     ]
    }
   ],
   "source": [
    "# Veryfing the results\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def predict(*, x: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    return logit(x @ w)\n",
    "\n",
    "def mean_cross_entropy(*, y: np.ndarray, pred: np.ndarray) -> float:\n",
    "    return -np.where(y, np.log(pred + 1e-8), np.log(1 - pred + 1e-8)).mean()\n",
    "\n",
    "frequentist_predictions = predict(x=X_train, w=w_mle)\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_train, frequentist_predictions > 0.5))\n",
    "print(f\"mean cross entropy: {mean_cross_entropy(y=y_train, pred=frequentist_predictions):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian Inference, we want to compute the **posterior distribution**:\n",
    "$$p(\\textbf{w} | \\mathbf{X}, Y) \\propto p(Y | \\textbf{w}, \\mathbf{X})p(\\textbf{w})$$\n",
    "\n",
    "\n",
    "where $p(\\textbf{w})$ is the prior, and the **predictive distribution**:\n",
    "$$p(y | \\mathbf{x}, \\mathbf{X}, Y) = \\int p(y | \\mathbf{x}, \\textbf{w}) p(\\textbf{w} | \\mathbf{X}, Y) d\\textbf{w}$$\n",
    "\n",
    "notice that $\\mathbf{X}$ is always in the conditioning, we are not interested in its distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior distribution\n",
    "\n",
    "For the posterior $p(\\textbf{w} | \\mathbf{X}, Y) \\propto p(Y | \\textbf{w}, \\mathbf{X})p(\\textbf{w})$ we resort to the **Laplace Approximation**:\n",
    "\n",
    "$$p(\\textbf{w} | \\mathbf{X}, Y) \\approx \\mathcal{N}(\\textbf{w}; \\mu_D, \\Sigma_D)$$\n",
    "\n",
    "where $\\mu_D = \\textbf{w}_{MAP}$ and $\\Sigma_D =  \\left( - \\nabla_w \\nabla_w \\log{p(\\textbf{w}_{MAP} | Y , \\mathbf{X})} \\right)^{-1}$\n",
    "\n",
    "where $\\textbf{w}_{MAP} = \\argmax_{\\textbf{w}} \\left[ p(\\textbf{w} | \\mathbf{X}, Y) \\right]$ is the **maximum a posteriori**, equivalently $\\textbf{w}_{MAP} = \\argmax_{\\textbf{w}}  \\left[ \\log{p(Y | \\textbf{w}, \\mathbf{X})} + \\log{p(\\textbf{w})} \\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can assume a Gaussian prior for the weights, with zero mean and a certain diagonal covariance matrix\n",
    "from torch.distributions import Normal\n",
    "\n",
    "PRIOR_STD = 100\n",
    "\n",
    "def log_prior(w):\n",
    "    return Normal(0, PRIOR_STD).log_prob(w).sum()\n",
    "\n",
    "def log_unnormalized_posterior(*, X, y, w):\n",
    "    return log_likelihood(X=X, y=y, w=w) + log_prior(w)\n",
    "\n",
    "loss_function = lambda w: -(log_unnormalized_posterior(X=X_tensor, y=y_tensor, w=w))\n",
    "\n",
    "w_map = gradient_descent_optimization(loss_function=loss_function,\n",
    "                                      initial_guess=np.zeros(X_train.shape[1]),\n",
    "                                      lr=1e-2,\n",
    "                                      n_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hessian(*, f, w):\n",
    "    w.requires_grad = True\n",
    "    grad = torch.autograd.grad(f(w), w, create_graph=True)[0]\n",
    "    hessian = torch.zeros((w.numel(), w.numel()))\n",
    "    for i in range(w.numel()):\n",
    "        hessian[i] = torch.autograd.grad(grad[i], w, retain_graph=True)[0]\n",
    "    w.requires_grad = False\n",
    "    return hessian\n",
    "\n",
    "hessian = compute_hessian(f=lambda w : log_unnormalized_posterior(X=X_tensor, y=y_tensor, w=w), w=torch.tensor(w_map, dtype=torch.float32))\n",
    "posterior_cov = torch.inverse(-hessian).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictive distribution\n",
    "\n",
    "The approximation for the **predictive distribution** follows from the Laplace approximation of the posterior approximation (see lecture's notes for more details):\n",
    "$$p(y | \\mathbf{x}, \\mathbf{X}, Y) \\approx f\\left(\\kappa\\left(\\sigma_a^2\\right) \\mu_a\\right)$$\n",
    "\n",
    "where $\\kappa\\left(\\sigma_a^2\\right)=\\left(1+\\pi \\frac{\\sigma_a^2}{8}\\right)^{-\\frac{1}{2}}$ and $\\mu_a$ and $\\sigma_a^2$ are the mean and variance of the linear combination of the gaussian posterior with the observation $\\mathbf{x}$.\n",
    "\n",
    "So $\\mu_a = \\mathbf{\\mu}_{D}^T \\mathbf{x}$ and $\\sigma_a^2 = \\mathbf{x}^T \\Sigma_{D} \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_predictive_distribution(*, x, posterior_mean, posterior_cov):\n",
    "    mu = x @ posterior_mean # (n, d) @ (d, )\n",
    "    var = np.diag(x @ posterior_cov @ x.T)\n",
    "    #var = np.array([X[i] @ posterior_cov @ X[i].T for i in range(len(X))])\n",
    "    k = (1 + np.pi*var/8)**(-0.5)    \n",
    "    return logit(mu * k)\n",
    "\n",
    "bayesian_predictions = laplace_predictive_distribution(x=X_test, posterior_mean=w_map, posterior_cov=posterior_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {\"train\": X_train, \"test\": X_test}\n",
    "y = {\"train\": y_train, \"test\": y_test}\n",
    "\n",
    "freq_weights = w_map\n",
    "\n",
    "predictor = {\"frequentist\": lambda x : predict(x=x, w=freq_weights),\n",
    "             \"bayesian\": lambda x :  laplace_predictive_distribution(x=x, posterior_mean=w_map, posterior_cov=posterior_cov)}\n",
    "\n",
    "for data in ('train', 'test'):\n",
    "    for model in ('frequentist', 'bayesian'):\n",
    "        pred = predictor[model](X[data])\n",
    "        print(f\"{model} {data} mean cross entropy: {mean_cross_entropy(y=y[data], pred=pred):.2f}\")\n",
    "        cm = confusion_matrix(y[data], pred > 0.5) / len(y[data])\n",
    "        #print(f\"{model} {data} confusion matrix: \\n\", np.round(cm,2))\n",
    "        print(f\"{model} {data} accuracy: {(cm[0,0] + cm[1,1]):.1%}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "def plot_train_data(X, y):\n",
    "    plt.scatter(X[y==1][:,1], X_train[y==1][:,2], c = 'red', marker='x', label='1')\n",
    "    plt.scatter(X[y==0][:,1], X_train[y==0][:,2], c = 'red', marker='o', label='0')\n",
    "    plt.legend()\n",
    "\n",
    "def binary_crossentropy(*, true: np.ndarray, pred: np.ndarray):\n",
    "    return -np.where(true, np.log(pred + 1e-8), np.log(1 - pred + 1e-8)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:,1], X_test[:,2], c = y_test, alpha = 0.8)\n",
    "plot_train_data(X_train, y_train)\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = laplace_predictive_distribution(x=X_test, posterior_mean=w_map, posterior_cov=posterior_cov)\n",
    "error = binary_crossentropy(true=y_test, pred=pred)\n",
    "plt.scatter(X_test[:,1], X_test[:,2], c = pred, alpha = 0.8)\n",
    "plot_train_data(X_train, y_train)\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(x=X_test, w=freq_weights)\n",
    "error = binary_crossentropy(true=y_test, pred=pred)\n",
    "plt.scatter(X_test[:,1], X_test[:,2], c = pred, alpha = 0.8)\n",
    "plot_train_data(X_train, y_train)\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to repeat the analysis using more points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try to repeat the analysis on the following dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
